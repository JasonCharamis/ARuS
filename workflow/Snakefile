import os
import re
import subprocess
import itertools

###=============================================================== Snakemake Utils Functions =====================================================================###

def is_docker() -> bool:
    with open('/proc/self/cgroup', 'r') as procfile:
        result = subprocess.run(["grep", "container"], stdin=procfile, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

        if result.returncode == 0:
            return True
        else:
            return False

    return False


def find_repository_name(start_dir="."):
    current_dir = os.path.abspath(start_dir)

    while current_dir != '/':  # Stop searching at the root directory
        result = subprocess.run(["find", current_dir, "-type", "f", "-name", "Snakefile"], capture_output=True, text=True)

        if result.stdout:
            snakefiles = result.stdout.strip().split('\n')
            if len(snakefiles) == 1:
                return snakefiles[0]
            else:
                print("Multiple repositories identified:")
                for snakefile in snakefiles:
                    print(f"- {snakefile}")

        current_dir = os.path.dirname(current_dir)

    # Of course, if a different path is provided with the --snakefile argument, this will be used by Snakemake
    return None  # Return None if no Snakefile or snakefile is found


def find_workflow_path(dir="."):
    home_directory = os.path.expanduser("~")
    repository_name = find_repository_name(dir)
    result = subprocess.run(["find", home_directory, "-type", "d", "-name", repository_name], capture_output=True, text=True)
    return result.stdout


###=============================================================== Snakemake Pipeline =========================================================================###

if is_docker():
     configfile: str( re.sub ("workflow.*","", find_repository_name(start_dir=".")) ) + "/config/config.yaml"
else:
     configfile: str( find_workflow_path(dir=".")) + "config/config.yaml"

## Wildcards
samples = sorted(list({ re.sub(r"\s+","", f[:-11]) for f in os.listdir(config['read_dir']) if f.endswith(".fastq.gz")}))
wildcard_constraints: samples = "trim"

groups = list(set(samples))

def all_pairs(x):
    comparisons = {}

    for sample1, sample2 in itertools.combinations(x, 2):
        comparisons[str(sample1 + "_" + sample2)] = str(sample1 + "_vs_" + sample2)

    return list(comparisons.values())

de_subset = all_pairs(groups)

#==================================================================== RULE ALL ============================================================================#

rule all:
    input:
        expand ("edgeR/02_analyze_DE/{de_subset}.P1e-3_C{log2FC_cutoff}.DE.annotated.sorted.xlsx", de_subset = de_subset, log2FC_cutoff=config['log2FC_cutoff'] )

rule fastqc:
    input:
        r1='{samples}_1.fastq.gz',
        r2='{samples}_2.fastq.gz'

    output: "fastqc/{samples}_fastqc/fastqc_report.html"
    conda: "envs/rnaseq.yaml"
    threads: 1

    shell:
        """ mkdir -p fastqc && 
	    fastqc {input.r1} -t {threads} && 
            fastqc {input.r2} -t {threads} """

rule trim_reads:
    input:
        r1='{samples}_1.fastq.gz',
        r2='{samples}_2.fastq.gz'

    output:
        r1_trimmed='{samples}_1.trimmed.fastq.gz',
        r2_trimmed='{samples}_2.trimmed.fastq.gz',
        r1_garbage=temp('{samples}_1.garbage.fastq.gz'),
        r2_garbage=temp('{samples}_2.garbage.fastq.gz')

    conda: "envs/rnaseq.yaml"
    threads: config['trimmomatic_threads']
    message: "Adapter-trimming reads"
    shell:
        """ trimmomatic PE -threads {threads} {input.r1} {input.r2} {output.r1_trimmed} {output.r1_garbage} {output.r2_trimmed} {output.r2_garbage}
            ILLUMINACLIP:$(find $(conda info --envs | awk '$1=="*"{print $NF}') -name TruSeq3-PE.fa):2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50 """


#================================================================ MAPPING =================================================================#

rule build_genome_index:
    output: 'index_chkp'
    conda: "envs/rnaseq.yaml"
    threads: config['star_build_threads']
    message: "Building genome index"
    params:
        genome_fasta=config['genome_fasta'],
        gtf=config['gtf']
    shell:
        """ star --runThreadN {threads} 
                 --runMode genomeGenerate
		 --genomeDir genome/ 
		 --genomeFastaFiles {params.genome_fasta}
		 --sjdbGTFfile {params.gtf}
		 --sjdbOverhang 149 &&

            mkdir genome/  && 
            touch index_chkp """

rule gunzip:
    input:
        '{samples}_1.trimmed.fastq.gz',
        '{samples}_2.trimmed.fastq.gz'

    output:
        '{samples}_1.trimmed.fastq',
        '{samples}_2.trimmed.fastq'

    conda: "envs/rnaseq.yaml"
    shell: """ gunzip {input.r1} &&
    	       gunzip {input.r2} """


rule mapping:
    input: r1_trimmed='{samples}_1.trimmed.fastq', r2_trimmed='{samples}_2.trimmed.fastq'
    output: '{samples}.Aligned.sortedByCoord.out.bam', '{samples}.chkpM'
    conda: "envs/rnaseq.yaml"
    message: "Mapping reads to genome and convert to sorted BAM"
    threads: config['star_map_threads']


    shell: """ star --runThreadN {threads}
                    --readFilesIn {input.r1_trimmed} {input.r2_trimmed}
                    --genomeDir genome/
                    --outSAMtype BAM SortedByCoordinate
                    --outFileNamePrefix {samples}. && touch {samples}.chkpM """


#=============================================================== COUNT =================================================================#

rule count:
    input: bams = expand('{samples}.Aligned.sortedByCoord.out.bam', samples=samples)
    output: counts='counts.txt'
    conda: "envs/rnaseq.yaml"
    threads: config['featureCounts_threads']
    params: gtf=config['gtf']
    shell:
        """ featurecounts -M 
                          -s 0 
                          -T {threads} 
                          -p 
                          -t exon 
                          -g gene_id 
                          -a {params.gtf} 
                          -o {output.counts} {input.bams} """

rule modify:
    input: counts="counts.txt"
    output: counts_mod="counts.mod.txt"
    shell:
        """ perl counts_mod.pl {input.counts} > {output.counts_mod} """

rule samples_list:
    input: counts_mod="counts.mod.txt"
    output: samples_list="samples.list"
    shell:
        """ perl counts_to_samples_list.pl {input.counts_mod} | 
            sort -Vk2 > {output.samples_list}  """

rule counts_to_tpm:
    input: 
        counts="counts.txt",
        file="samples.list"

    output:
        counts_mod_tpm="counts.mod.tpm"

    shell:
        """ perl counts_to_tpm.pl {input.counts} |
            sed 's/\\.Aligned\\.sortedByCoord\\.out\\.bam//g' |
            sed 's/gene://g' |
            sed 's/results\\///g' > {output.counts_mod_tpm} """

rule pca:
    input: counts_tpm="counts.mod.tpm"
    output: "PCA.svg"
    shell:
        """ Rscript pca.R {input.counts_tpm} """

#================================================================ RUN DE ANALYSIS =================================================================#

rule make_directories:
    input:
        samples_list="samples.list",
        pca="PCA.svg",
        counts="counts.txt"

    output: 'chkp'

    shell:
        """  mkdir edgeR &&
             cd edgeR &&
             mkdir 01_run_DE_analysis &&
             mkdir 02_analyze_DE &&
             cd ../ &&
             cp counts.mod.txt edgeR/01_run_DE_analysis/ &&
             cp {input.samples_list} edgeR/01_run_DE_analysis/ &&
             touch chkp """

rule run_DE_analysis:
    input:
        counts_file="counts.mod.txt",
        samples_list="samples.list",
        chkp='chkp'
	
    output: 'edgeR/chkp01'
    shell:
        """  cd edgeR/01_run_DE_analysis &&
             perl run_DE_analysis.pl --matrix ../../../{input.counts_file} --method edgeR --samples_file ../../{input.samples_list} &&
             cd ../ &&
             touch chkp01 """

rule analyze_DE:
    input: 'edgeR/chkp01'
    output: 'edgeR/chkp02'
    params: DE_cutoff=config['log2FC_cutoff']
    shell:
        """ cd edgeR/02_analyze_DE &&
             ln -s ../01_run_DE_analysis/edgeR.*/counts.mod.txt* . &&
             perl analyze_diff_expr.pl --matrix ../../counts.mod.txt --samples ../01_run_DE_analysis/samples.list -P 1e-3 -C {params.DE_cutoff} &&
             cd ../ &&
             touch chkp02 &&
             rm ../chkp ../{input} ../{output} """


rule rename:
      input:  subset_file='edgeR/02_analyze_DE/counts.mod.txt.{de_subset}.edgeR.DE_results.P1e-3_C2.DE.subset'
      output: subset_file_renamed='edgeR/02_analyze_DE/{de_subset}.P1e-3_C2.DE.subset'
      shell: """ perl rename.pl {input.subset_file} """


#================================================================ POST DE ANNOTATION =================================================================#

rule annotate_DE:
     input: de_file="edgeR/02_analyze_DE/{de_subset}.P1e-3_C{config['log2FC_cutoff']}.DE.subset", gene_info=config['gene_info']
     output: annotated="edgeR/02_analyze_DE/{de_subset}.P1e-3_C{config['log2FC_cutoff']}.DE.annotated.tsv"
     shell: """ perl annotate_DE.pl {input.de_file} 
     	    	     		    {input.gene_info} > {output.annotated} """


rule reverse_sort:
     input:  unsorted = "edgeR/02_analyze_DE/{de_subset}.P1e-3_C{config['log2FC_cutoff']}.DE.annotated.tsv"
     output: sorted = "edgeR/02_analyze_DE/{de_subset}.P1e-3_C{config['log2FC_cutoff']}.DE.annotated.sorted.tsv"
     shell: """ perl reverse_sort.pl {input.unsorted} > {output.orthology_sorted} """


rule tsv2xlsx:
     input: tsv="edgeR/02_analyze_DE/{de_subset}.P1e-3_C{config['log2FC_cutoff']}.DE.annotated.sorted.tsv"
     output: "edgeR/02_analyze_DE/{de_subset}.P1e-3_C{config['log2FC_cutoff']}.DE.annotated.sorted.xlsx"
     shell: """ python3 tsv2xlsx.py {input.tsv} """